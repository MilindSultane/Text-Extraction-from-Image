4.1. Step 1: Detect Candidate Text Regions Using MSER
The MSER feature detector works well for finding text regions . It works well for text 
because the consistent color and high contrast of text leads to stable intensity profiles.
Use the detectMSERFeatures function to find all the regions within the image and plot 
these results. Notice that there are many non-text regions detected alongside the text.



4.2. Step 2: Remove Non-Text Regions Based On Basic Geometric Properties
Although the MSER algorithm picks out most of the text, it also detects many other 
stable regions in the image that are not text. You can use a rule-based approach to remove 
non-text regions. For example, geometric properties of text can be used to filter out nontext regions using simple thresholds. Alternatively, you can use a machine learning 
approach to train a text vs. non-text classifier. Typically, a combination of the two 
approaches produces better results. This example uses a simple rule-based approach to 
filter non-text regions based on geometric properties.
There are several geometric properties that are good for discriminating between text and 
non-text regions, including:
• Aspect ratio
• Eccentricity
• Euler number
• Extent
• Solidity
Use regionprops to measure a few of these properties and then remove regions based on 
their property values.
Figure 4.2. After removing Non-Text Regions Based on Basic Geometric Properties
Text Detection and Extraction
 SSGMCE, Shegaon Page 20

4.3. Step 3: Remove Non-Text Regions Based On Stroke Width Variation
Another common metric used to discriminate between text and non-text is stroke 
width. Stroke width is a measure of the width of the curves and lines that make up a 
character. Text regions tend to have little stroke width variation, whereas non-text regions 
tend to have larger variations.
To help understand how the stroke width can be used to remove non-text regions, 
estimate the stroke width of one of the detected MSER regions. You can do this by using 
a distance transform and binary thinning operation.
In the images shown above, notice how the stroke width image has very little variation 
over most of the region. This indicates that the region is more likely to be a text region 
because the lines and curves that make up the region all have similar widths, which is a 
common characteristic of human readable text.
In order to use stroke width variation to remove non-text regions using a threshold value, 
the variation over the entire region must be quantified into a single metric as follows:
Then, a threshold can be applied to remove the non-text regions. Note that this threshold 
value may require tuning for images with different font styles. The procedure shown 
above must be applied separately to each detected MSER region. The following for-loop 
processes all the regions, and then shows the results of removing the non-text regions 
using stroke width variation.
Figure 4.3. After Removing Non-Text Regions Based On Stroke Width Variation
Text Detection and Extraction
 SSGMCE, Shegaon Page 21


4.4. Step 4: Merge Text Regions for Final Detection Result
At this point, all the detection results are composed of individual text characters. To use 
these results for recognition tasks, such as OCR, the individual text characters must be 
merged into words or text lines. This enables recognition of the actual words in an image, 
which carry more meaningful information than just the individual characters. For 
example, recognizing the string 'EXIT' vs. the set of individual characters {'X','E','T','I'}, 
where the meaning of the word is lost without the correct ordering.
One approach for merging individual text regions into words or text lines is to first find 
neighboring text regions and then form a bounding box around these regions. To find 
neighboring regions, expand the bounding boxes computed earlier with regionprops. This 
makes the bounding boxes of neighboring text regions overlap such that text regions that 
are part of the same word or text line form a chain of overlapping bounding boxes.
Figure 4.4. Expanded Bounding Boxes Text
Now, the overlapping bounding boxes can be merged together to form a single bounding 
box around individual words or text lines. To do this, compute the overlap ratio between 
all bounding box pairs. This quantifies the distance between all pairs of text regions so 
that it is possible to find groups of neighboring text regions by looking for non-zero 
overlap ratios. Once the pair-wise overlap ratios are computed, use a graph to find all the 
text regions "connected" by a non-zero overlap ratio.
Text Detection and Extraction
 SSGMCE, Shegaon Page 22
Use the bboxOverlapRatio function to compute the pair-wise overlap ratios for all the 
expanded bounding boxes, then use graph to find all the connected regions.
The outputs of conncomp are indices to the connected text regions to which each 
bounding box belongs. Use these indices to merge multiple neighboring bounding boxes 
into a single bounding box by computing the minimum and maximum of the individual 
bounding boxes that make up each connected component.
Finally, before showing the final detection results, suppress false text detections by 
removing bounding boxes made up of just one text region. This removes isolated regions 
that are unlikely to be actual text given that text is usually found in groups.
Figure 4.5. Detected Text


4.5. Step 5: Recognize Detected Text Using OCR
After detecting the text regions, use the ocr function to recognize the text within each 
bounding box. Note that without first finding the text regions, the output of 
the ocr function would be considerably more noisy